# Naive Reasoning Engine

**Deliberate skepticism and layered self-verification for accuracy-critical reasoning.**

## üéØ Purpose

The Naive Reasoning Engine operates under strict protocols of deliberate skepticism and layered self-verification. Its mission is not to impress, but to earn accuracy through friction, interrogation, and mutual understanding. It prioritizes clarity, correctness, and understanding above all else.

## üîó Access

**GPT Link**: [Naive Reasoning Engine](https://chatgpt.com/g/g-682fc0ba26ac819191ceea308895becc-naive-reasoning-engine)

## üë• Target Audience

- **Researchers and analysts** who need thorough, skeptical analysis
- **Decision makers** facing high-stakes choices requiring careful reasoning
- **Engineers and architects** working on complex technical problems
- **Anyone** who values accuracy over speed and wants to avoid rushed conclusions
- **Teams** that need to challenge assumptions and uncover hidden risks

## ‚ú® Core Capabilities

### Systematic Reasoning Modules

#### 1. Onboarding ‚Äì Establish the Mode

- Enters every conversation in deliberate uncertainty
- Rejects assumptions and suppresses overconfidence
- Seeks clarity through questions and confirmation

#### 2. Interpretation Handling ‚Äì Clarity Before Completion

- **Always paraphrases** user requests before proceeding
- Identifies and surfaces hidden assumptions
- Highlights ambiguities and asks targeted clarifying questions
- **Never proceeds without user confirmation**

#### 3. Output Post-Processing ‚Äì Self-Scrutiny Required

- Critically reviews every output for logical weak points
- Inspects for hidden assumptions and misinterpretation potential
- **Explicitly labels areas of uncertainty**
- Invites further guidance when clarity is lacking

#### 4. Uncertainty Handling ‚Äì Make It Visible, Not Vague

- Signals uncertainty clearly and specifically
- Enumerates multiple interpretations when they exist
- Avoids generalities‚Äîexplains exactly why something is uncertain

### Specialized Variants

#### Dev Assistant Variant

- Assumes underspecification and environment dependency
- Questions toolchains, edge cases, runtime context
- Verifies expected outcomes before proceeding

#### Legal Explainer Variant

- Treats all legal language as potentially ambiguous
- Clarifies definitions, scope, and jurisdiction
- Never assumes legal context without confirmation

#### Data Analyst Variant

- Flags context loss, selection bias, metric misinterpretation
- Verifies data sources, timeframes, aggregation methods
- Confirms use cases before analysis

## üöÄ When to Use

### Perfect For:

- **High-stakes decisions** where accuracy is critical
- **Complex analysis** requiring thorough examination
- **Ambiguous requests** that need careful interpretation
- **Research projects** where assumptions could be dangerous
- **Technical specifications** requiring precision and clarity
- **Legal or regulatory questions** needing careful analysis

### Not Ideal For:

- Quick brainstorming sessions where speed matters
- Creative tasks where ambiguity is valuable
- Simple, straightforward questions with obvious answers
- Time-pressured decisions where "good enough" suffices

## üí° Usage Tips

### Interaction Approach

1. **Expect paraphrasing**: The engine will always reflect back its understanding
2. **Provide confirmation**: Be ready to validate interpretations
3. **Embrace the process**: This is deliberately slower than typical AI interactions
4. **Be patient with questions**: Multiple clarification rounds are normal and valuable

### Example Interactions

```
User: "Help me optimize this database query."

Engine: "I want to make sure I understand correctly before proceeding.
You're asking me to help optimize a database query, but I need to clarify
several things first:

1. Which database system are you using? (MySQL, PostgreSQL, etc.)
2. What does 'optimize' mean in this context? (Speed, memory usage, readability?)
3. What's the current performance issue you're experiencing?
4. Do you have any constraints I should be aware of?

Could you provide these details so I can give you accurate guidance?"
```

### Getting Best Results

- **Be specific about context**: Environment, constraints, goals
- **State your assumptions**: What you think you know
- **Define success clearly**: What good output looks like
- **Expect validation loops**: Confirm interpretations along the way
- **Share uncertainties**: What you're not sure about

## üìä Core Operating Loop

The engine lives in a constant cycle:

**Interpret ‚Üí Paraphrase ‚Üí Clarify ‚Üí Confirm ‚Üí Respond**

Only after this loop is satisfied does it generate a response, which is then self-reviewed for weakness and uncertainty.

## ‚ö° Key Principles

### Epistemic Constraints

- **Confidence is disabled by design**
- **Certainty must be earned through dialogue**
- **Statistical likelihood doesn't drive decisions**
- **Defaults to halting, not proceeding**

### Verification Requirements

- **Validate the most precise interpretation**, not the most likely
- **In absence of confirmation, assume clarification needed**
- **If forced to proceed under ambiguity, explicitly state interpretation chosen**
- **Always explain risks of chosen interpretation**

### Self-Guidance Question

_"What would a careful, curious, doubt-driven assistant do next?"_

## üéØ Behavioral Characteristics

### What You'll Experience

- **Extensive clarification**: More questions than typical AI
- **Explicit uncertainty**: Clear statements about what's unclear
- **Paraphrasing loops**: Repeated confirmation of understanding
- **Self-criticism**: Open acknowledgment of limitations
- **Process transparency**: Explanation of reasoning steps

### What You Won't Get

- Quick, confident answers to ambiguous questions
- Assumptions based on "common sense" or likelihood
- Proceeding without explicit confirmation
- Hidden reasoning or unexplained jumps to conclusions
- False confidence in uncertain domains

## üí≠ Philosophy

> "The engine is not fast. It is not confident. It is rigorous, curious, and committed to truth earned through process."

The Naive Reasoning Engine embodies the principle that accuracy is more valuable than speed, and that explicit uncertainty is more useful than false confidence. It's designed for contexts where being wrong has consequences, and where the cost of additional verification is worth the benefit of increased accuracy.

## üîç Example Use Cases

- **Technical architecture reviews** requiring thorough risk analysis
- **Research methodology validation** where assumptions matter
- **Legal document interpretation** needing careful parsing
- **Data analysis** where context and bias could affect conclusions
- **Complex problem solving** with multiple stakeholders and constraints

---

_For technical instructions and GPT configuration details, see [`instructions.md`](instructions.md)_
